{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the performance of the base model\n",
    "\n",
    "In this notebook, we will test the performance of the base model. The test is performed by predicting the similarity between the support dataset and test dataset. \n",
    "\n",
    "The support dataset are the selected cutouts the based model was trained on, i.e. the first 13 cutouts.\n",
    "\n",
    "The test dataset are the rest of of cutouts which was not used for training.\n",
    "\n",
    "Both the selected cutouts and the optimized base model are available on Zenodo:\n",
    "\n",
    "- Selected cutouts: [link](https://zenodo.org/records/13829957/files/cutouts.zip?download=1)\n",
    "- Optimized base models (deep and shallow): [link](https://zenodo.org/records/13829957/files/optimized_model.zip?download=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from keras import backend as k\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import LearningRateScheduler, ReduceLROnPlateau, Callback\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the support data and test data\n",
    "\n",
    "data_dir = Path(\"../data/selected_cutouts/\")\n",
    "list_samples_file = [\n",
    "    \"label142377591163_murumuru.zarr\",\n",
    "    \"label244751236943_tucuma.zarr\",\n",
    "    \"label174675723264_banana.zarr\",\n",
    "    \"label999240878592_cacao.zarr\",\n",
    "    \"label370414265344_fruit.zarr\"\n",
    "]\n",
    "\n",
    "support_samples = None\n",
    "test_samples = None\n",
    "for file in list_samples_file:\n",
    "    ds_samples = xr.open_zarr(data_dir / file)\n",
    "    n_sample = ds_samples.sizes[\"sample\"]\n",
    "    if support_samples is None:\n",
    "        support_samples = ds_samples.isel(sample=range(13))\n",
    "    else:\n",
    "        support_samples = xr.concat([support_samples, ds_samples.isel(sample=range(13))], dim=\"sample\")\n",
    "    if n_sample>13:\n",
    "        if test_samples is None:\n",
    "            test_samples = ds_samples.isel(sample=range(13, n_sample))\n",
    "        else:\n",
    "            test_samples = xr.concat([test_samples, ds_samples.isel(sample=range(13, n_sample))], dim=\"sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the refined model\n",
    "@keras.saving.register_keras_serializable(package=\"MyLayers\")\n",
    "class euclidean_lambda(keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(euclidean_lambda, self).__init__(**kwargs)\n",
    "        self.name = 'euclidean_lambda'\n",
    "\n",
    "    def call(self, featA, featB):\n",
    "        squared = keras.ops.square(featA-featB)\n",
    "        return squared\n",
    "\n",
    "# Base model, the model trained only using the initial data\n",
    "base_model_path = '../data/deep_model/siamese_model_mobilenet03.keras'\n",
    "base_model = keras.saving.load_model(base_model_path)\n",
    "\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compute the classification performance\n",
    "def predict_label(gt_label, score_dic, metric=\"max\", k=3):\n",
    "    \"\"\"\n",
    "    gt_label: ground truth label\n",
    "    score_dic: the dic that contains the predicted similarity score for each support sample\n",
    "               the key is the class label\n",
    "    metric: the metric to aggregate the similarity scores across the support samples within each class\n",
    "    \n",
    "    return:\n",
    "        result: [ifcorrect, similarity_score_of_the_target_class, predicted_class, similarity_score_of_the_predicted_class]\n",
    "    \"\"\"\n",
    "    reduced_score = {}\n",
    "    for key, values in score_dic.items():\n",
    "        if metric == \"avg\": \n",
    "            reduced_score[key] = sum(values) / len(values) if values else 0\n",
    "        elif metric == \"max\":\n",
    "            reduced_score[key] = max(values) if values else 0\n",
    "        elif metric == \"knn\":\n",
    "            reduced_score[key] = np.sort(values)[-k:] # get the top k scores for each class\n",
    "\n",
    "    gt_label = int(gt_label)\n",
    "    if metric == \"knn\":\n",
    "        reversed_dict = {vi: k for k, v in reduced_score.items() for vi in v}\n",
    "        top_k_scores_all = np.concatenate(list(reduced_score.values()))\n",
    "        top_k_scores_all_sorted = np.sort(top_k_scores_all)[::-1][0:k]\n",
    "        top_k_classes = [reversed_dict[key] for key in top_k_scores_all_sorted]\n",
    "        counter = Counter(top_k_classes)\n",
    "        largest_key = counter.most_common(1)[0][0]\n",
    "        largest_value = top_k_scores_all_sorted.mean()\n",
    "        largesr_value_gt = reduced_score[gt_label].mean()\n",
    "    else:\n",
    "        largest_key = max(reduced_score, key=reduced_score.get)\n",
    "        largest_value = reduced_score[largest_key]\n",
    "        largesr_value_gt = reduced_score[gt_label]\n",
    "    \n",
    "    \n",
    "    if gt_label==largest_key:\n",
    "        result = [1, gt_label, largest_value, largest_key, largest_value]\n",
    "    else:\n",
    "        result = [0, gt_label, largesr_value_gt, largest_key, largest_value]\n",
    "             \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the similarity score with each support sample and sort the similarity scores by class\n",
    "def get_similarity_score(test_X, support_samples, model):   \n",
    "    support_X = support_samples[\"X\"] / 255.0  \n",
    "    similarity_score = model.predict([test_X, support_X], verbose=0).squeeze()\n",
    "\n",
    "    # store the score into each class dic\n",
    "    unique_labels = np.unique(support_samples['Y'].values)\n",
    "    score_dic = {int(unique_label):[] for unique_label in unique_labels}\n",
    "    for j, support_Y in enumerate(support_samples['Y'].values):\n",
    "        score_dic[support_Y].append(similarity_score[j])\n",
    "    \n",
    "    return score_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_metric = \"knn\" # \"avg\" or \"max\" or \"knn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the results\n",
    "base_results = np.zeros((0, 5))\n",
    "\n",
    "num_test_samples = test_samples.sizes[\"sample\"]\n",
    "for j in range(num_test_samples):\n",
    "    test_sample_j = test_samples.isel(sample=j)\n",
    "\n",
    "    # Make the batch size as the total support_sample size\n",
    "    support_sample_size = len(support_samples[\"X\"][\"sample\"])\n",
    "    test_Y = test_sample_j[\"Y\"].values\n",
    "    test_X = test_sample_j.expand_dims({\"sample\": support_sample_size})[\"X\"] / 255.0\n",
    "\n",
    "    # Test the base model zero-shot learning\n",
    "    zeroshot_score_dic = get_similarity_score(test_X, support_samples, base_model)\n",
    "\n",
    "    # Compute the prediction results\n",
    "    zeroshot_result_j = predict_label(test_Y, zeroshot_score_dic, metric=predict_metric, k=3)\n",
    "    print(\"zero shot\", zeroshot_result_j)\n",
    "\n",
    "    base_results = np.vstack((base_results, zeroshot_result_j))\n",
    "\n",
    "print(\"-\" * 20)\n",
    "print(\n",
    "    \"Overall accuracy of the base model\",\n",
    "    sum(base_results[:, 0]) / num_test_samples,\n",
    ")\n",
    "\n",
    "gt_zero_shot = base_results[:, 1]\n",
    "pd_zero_shot = base_results[:, 3]\n",
    "base_results = classification_report(gt_zero_shot, pd_zero_shot)\n",
    "cm = confusion_matrix(gt_zero_shot, pd_zero_shot)\n",
    "print(\"***** zero shot results *****\")\n",
    "print(base_results)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| k-shot n-Fold | support set | test set     | n-Accuracy  |0-Accuracy  | \n",
    "| :---   |    :---              |        :---             | :---   |  :---   |\n",
    "| 3-1      | (0, 1, 2)                |   (3, 4, 5)                       |  0.89   |  |\n",
    "| 3-2      | (3, 4, 5)                      | (0, 1, 2)                        | 0.83    | |\n",
    "| 2-1      | (0, 1)                      | (2, 3, 4, 5)                         | 0.61    |0.22 |\n",
    "| 2-2      | (2, 3)                      | (0, 1, 4, 5)                         | 0.56    |0.22  |\n",
    "| 2-3      | (4, 5)                      | (0, 1, 2, 3)                         | 0.56    |0.22  |\n",
    "| 1-1      | (0)                      | (1, 2, 3, 4, 5)                         | 0.61    |0.31  |\n",
    "| 1-2      | (1)                      | (0, 1, 2, 3, 4)                         | 0.56    |0.28  |\n",
    "| 1-3      | (2)                      | (0, 1, 3, 4, 5)                         | 0.50    |0.22  |\n",
    "| 1-4      | (3)                      | (0, 1, 2, 4, 5)                         | 0.56    |0.11  |\n",
    "| 1-5      | (4)                      | (0, 1, 2, 3, 5)                         | 0.06    |0.00  |\n",
    "| 1-6      | (5)                      | (0, 1, 2, 3, 4)                         | 0.61    |0.11  |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EnvSiamense",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
