{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import xarray as xr\n",
    "from sklearn import svm\n",
    "from pathlib import Path\n",
    "from keras import backend  # required for loading model\n",
    "from keras.models import load_model\n",
    "import dask.array as da\n",
    "\n",
    "import dask\n",
    "dask.config.set(scheduler='synchronous')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and Siamise network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = xr.open_zarr('data/svm_training_data.zarr')\n",
    "imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load example classes\n",
    "class_files = sorted([file for file in Path('data/example_classes/').rglob('*.zarr')])\n",
    "# Manually create a map between class text and integer label\n",
    "class_map = {0: 'banana', 1:'cacao', 2:'fruit', 3:'palmtree'}\n",
    "class_data={}\n",
    "for class_i in range(len(class_files)):\n",
    "    class_data[class_i] = xr.open_zarr(class_files[class_i])\n",
    "class_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Siamese network\n",
    "siamese_model = load_model('../optimized_models/siamese_model.h5')\n",
    "siamese_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute similarity matrix for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to memory limit, we make a function to compute the similarity score per batch\n",
    "batch_size = 10  # number of samples to process at once to compute similarity score\n",
    "def predict_per_chunk(x, y):\n",
    "    \"\"\"Compute similarity score between two sets of images in the same bacth.\"\"\"\n",
    "    return siamese_model.predict([x, y], verbose=0).squeeze()\n",
    "\n",
    "# Compute similarity score for each class per sample\n",
    "matrix_stats = np.empty((0, 4*len(class_map))) # each row: mean, std, max, min per class \n",
    "for sample_i in range(imgs.sizes['sample']):\n",
    "    test_sample = imgs.isel(sample=sample_i)\n",
    "    arr_stats = np.empty((0))\n",
    "    print(f\"Processing sample {sample_i}\")\n",
    "    for class_i in class_map.keys():\n",
    "\n",
    "        # Make sample and example class data pairs\n",
    "        shape = class_data[class_i][\"sample\"].shape[0]\n",
    "        X_sample_norm = test_sample.expand_dims({\"sample\": shape})[\"X\"] / 255.0\n",
    "        X_class_norm = class_data[class_i][\"X\"] / 255.0\n",
    "\n",
    "        # Debug: select 10 samples\n",
    "        X_sample_norm = X_sample_norm.isel(sample=slice(0, 10))\n",
    "        X_class_norm = X_class_norm.isel(sample=slice(0, 10))\n",
    "\n",
    "        # Chunk the data\n",
    "        X_sample_norm = X_sample_norm.chunk({\"sample\": batch_size})\n",
    "        X_class_norm = X_class_norm.chunk({\"sample\": batch_size})\n",
    "\n",
    "        # Compute similarity scores per batch\n",
    "        scores = da.map_blocks(\n",
    "            predict_per_chunk,\n",
    "            X_sample_norm.data,\n",
    "            X_class_norm.data,\n",
    "            dtype=\"float32\",\n",
    "            chunks=(batch_size,),\n",
    "            drop_axis=(1, 2, 3),\n",
    "        )\n",
    "\n",
    "        scores = scores.compute()\n",
    "\n",
    "        # mean, std, max, min\n",
    "        statistics = np.array([scores.mean(), scores.std(), scores.max(), scores.min()])\n",
    "        arr_stats = np.hstack((arr_stats, statistics))\n",
    "    matrix_stats = np.vstack((matrix_stats, arr_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save matrix_stats to npy\n",
    "np.save(\"data/matrix_stats.npy\", matrix_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load matrix_stats from npy\n",
    "matrix_stats = np.load(\"data/matrix_stats.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gaussian_kernel(mean_similarity, y):\n",
    "#     \"\"\"customized gaussian kernel function for SVM.\n",
    "#     \"\"\"\n",
    "#     distance = 1 - mean_similarity\n",
    "\n",
    "#     return np.exp(np.dot(distance,(-y.T)))\n",
    "\n",
    "# classifier = svm.SVC(kernel=gaussian_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = svm.SVC()\n",
    "mean_similarity = matrix_stats[:, 0::4]\n",
    "y = imgs['Y'].values\n",
    "classifier.fit(mean_similarity, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the SVM model as a pickle file\n",
    "with open('./svm_classifier.pkl', 'wb') as f:\n",
    "    pickle.dump(classifier, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mobyle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
