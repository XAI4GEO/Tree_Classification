{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the performance of the n-shot leanrning using the leave-one-out test set of each fold\n",
    "**Note**: each the based model\n",
    "In each k and each fold we do the following to test the performance of the siamese network:\n",
    "1. Load the support set and the test set for each k and each fold \n",
    "2. Select a sample from the test set, and predict the similarity score between it with each sample in the support set\n",
    "3. Repeat 2 for all the samples from the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from keras import backend as k\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import LearningRateScheduler, ReduceLROnPlateau, Callback\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the support data and test data\n",
    "\n",
    "data_dir = Path(\"../../../../data/cleaned_data/selected_cutouts/\")\n",
    "list_samples_file = [\n",
    "    \"label142377591163_murumuru.zarr\",\n",
    "    \"label244751236943_tucuma.zarr\",\n",
    "    \"label174675723264_banana.zarr\",\n",
    "    \"label999240878592_cacao.zarr\",\n",
    "    \"label370414265344_fruit.zarr\"\n",
    "]\n",
    "\n",
    "support_samples = None\n",
    "test_samples = None\n",
    "for file in list_samples_file:\n",
    "    ds_samples = xr.open_zarr(data_dir / file)\n",
    "    n_sample = ds_samples.sizes[\"sample\"]\n",
    "    if support_samples is None:\n",
    "        support_samples = ds_samples.isel(sample=range(13))\n",
    "    else:\n",
    "        support_samples = xr.concat([support_samples, ds_samples.isel(sample=range(13))], dim=\"sample\")\n",
    "    if n_sample>13:\n",
    "        if test_samples is None:\n",
    "            test_samples = ds_samples.isel(sample=range(13, n_sample))\n",
    "        else:\n",
    "            test_samples = xr.concat([test_samples, ds_samples.isel(sample=range(13, n_sample))], dim=\"sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples['Y'].plot.hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the refined model\n",
    "@keras.saving.register_keras_serializable(package=\"MyLayers\")\n",
    "class euclidean_lambda(keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(euclidean_lambda, self).__init__(**kwargs)\n",
    "        self.name = 'euclidean_lambda'\n",
    "\n",
    "    def call(self, featA, featB):\n",
    "        squared = keras.ops.square(featA-featB)\n",
    "        return squared\n",
    "\n",
    "# Base model, the model trained only using the initial data\n",
    "base_model_path = '../../optimized_models/results_training/Agu_pairs_training_v8/siamese_model_mobilenet03.keras'\n",
    "base_model = keras.saving.load_model(base_model_path)\n",
    "\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compute the classification performance\n",
    "def predict_label(gt_label, score_dic, metric=\"max\"):\n",
    "    \"\"\"\n",
    "    gt_label: ground truth label\n",
    "    score_dic: the dic that contains the predicted similarity score for each support sample\n",
    "               the key is the class label\n",
    "    metric: the metric to aggregate the similarity scores across the support samples within each class\n",
    "    \n",
    "    return:\n",
    "        result: [ifcorrect, similarity_score_of_the_target_class, predicted_class, similarity_score_of_the_predicted_class]\n",
    "    \"\"\"\n",
    "    reduced_score = {}\n",
    "    for key, values in score_dic.items():\n",
    "        if metric == \"avg\": \n",
    "            reduced_score[key] = sum(values) / len(values) if values else 0\n",
    "        elif metric == \"max\":\n",
    "            reduced_score[key] = max(values) if values else 0\n",
    "        \n",
    "    largest_key = max(reduced_score, key=reduced_score.get)\n",
    "    largest_value = reduced_score[largest_key]\n",
    "    gt_label = int(gt_label)\n",
    "    \n",
    "    if gt_label==largest_key:\n",
    "        result = [1, gt_label, largest_value, largest_key, largest_value]\n",
    "    else:\n",
    "        result = [0, gt_label, reduced_score[gt_label], largest_key, largest_value]\n",
    "             \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the similarity score with each support sample and sort the similarity scores by class\n",
    "def get_similarity_score(test_X, support_samples, model):   \n",
    "    support_X = support_samples[\"X\"] / 255.0  \n",
    "    similarity_score = model.predict([test_X, support_X], verbose=0).squeeze()\n",
    "\n",
    "    # store the score into each class dic\n",
    "    unique_labels = np.unique(support_samples['Y'].values)\n",
    "    score_dic = {int(unique_label):[] for unique_label in unique_labels}\n",
    "    for j, support_Y in enumerate(support_samples['Y'].values):\n",
    "        score_dic[support_Y].append(similarity_score[j])\n",
    "    \n",
    "    return score_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_metric = \"avg\" # \"avg\" or \"max\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the results\n",
    "base_results = np.zeros((0, 5))\n",
    "# refined_results = np.zeros((0, 5))\n",
    "# print(\n",
    "#     \"[ifcorrect, gt_label, similarity_score_of_the_target_class, predicted_class, similarity_score_of_the_predicted_class]\"\n",
    "# )\n",
    "\n",
    "num_test_samples = test_samples.sizes[\"sample\"]\n",
    "# num_test_samples = 18\n",
    "for j in range(num_test_samples):\n",
    "    test_sample_j = test_samples.isel(sample=j)\n",
    "\n",
    "    # Make the batch size as the total support_sample size\n",
    "    support_sample_size = len(support_samples[\"X\"][\"sample\"])\n",
    "    test_Y = test_sample_j[\"Y\"].values\n",
    "    test_X = test_sample_j.expand_dims({\"sample\": support_sample_size})[\"X\"] / 255.0\n",
    "\n",
    "    # ### Test the base model zero-shot learning\n",
    "    # # Compute the similarity scores across classes\n",
    "    zeroshot_score_dic = get_similarity_score(test_X, support_samples, base_model)\n",
    "\n",
    "    # # Compute the prediction results\n",
    "    zeroshot_result_j = predict_label(test_Y, zeroshot_score_dic, metric=predict_metric)\n",
    "    print(\"zero shot\", zeroshot_result_j)\n",
    "\n",
    "    # ### Test the refined model for n-shot learning\n",
    "    # # Compute the similarity scores across classes\n",
    "    # nshot_score_dic = get_similarity_score(test_X, support_samples, refined_model)\n",
    "\n",
    "    # # Compute the prediction results\n",
    "    # nshot_result_j = predict_label(test_Y, nshot_score_dic, metric=\"avg\")\n",
    "\n",
    "    # # All the results\n",
    "    # print(\"n-shot\", nshot_result_j)\n",
    "\n",
    "    # All the results\n",
    "    base_results = np.vstack((base_results, zeroshot_result_j))\n",
    "    # refined_results = np.vstack((refined_results, nshot_result_j))\n",
    "\n",
    "print(\"-\" * 20)\n",
    "print(\n",
    "    \"Overall accuracy of the base model\",\n",
    "    sum(base_results[:, 0]) / num_test_samples,\n",
    ")\n",
    "# print(\n",
    "#     \"Overall accuracy of the refined model\",\n",
    "#     sum(refined_results[:, 0]) / num_test_samples,\n",
    "# )\n",
    "\n",
    "gt_zero_shot = base_results[:, 1]\n",
    "pd_zero_shot = base_results[:, 3]\n",
    "base_results = classification_report(gt_zero_shot, pd_zero_shot)\n",
    "cm = confusion_matrix(gt_zero_shot, pd_zero_shot)\n",
    "print(\"***** zero shot results *****\")\n",
    "print(base_results)\n",
    "print(cm)\n",
    "# gt_n_shot = refined_results[:, 1]\n",
    "# pd_n_shot = refined_results[:, 3]\n",
    "# n_shot_results = classification_report(gt_n_shot, pd_n_shot)\n",
    "# cm = confusion_matrix(gt_n_shot, pd_n_shot)\n",
    "# print(\"***** n shot results *****\")\n",
    "# print(n_shot_results)\n",
    "# print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| k-shot n-Fold | support set | test set     | n-Accuracy  |0-Accuracy  | \n",
    "| :---   |    :---              |        :---             | :---   |  :---   |\n",
    "| 3-1      | (0, 1, 2)                |   (3, 4, 5)                       |  0.89   |  |\n",
    "| 3-2      | (3, 4, 5)                      | (0, 1, 2)                        | 0.83    | |\n",
    "| 2-1      | (0, 1)                      | (2, 3, 4, 5)                         | 0.61    |0.22 |\n",
    "| 2-2      | (2, 3)                      | (0, 1, 4, 5)                         | 0.56    |0.22  |\n",
    "| 2-3      | (4, 5)                      | (0, 1, 2, 3)                         | 0.56    |0.22  |\n",
    "| 1-1      | (0)                      | (1, 2, 3, 4, 5)                         | 0.61    |0.31  |\n",
    "| 1-2      | (1)                      | (0, 1, 2, 3, 4)                         | 0.56    |0.28  |\n",
    "| 1-3      | (2)                      | (0, 1, 3, 4, 5)                         | 0.50    |0.22  |\n",
    "| 1-4      | (3)                      | (0, 1, 2, 4, 5)                         | 0.56    |0.11  |\n",
    "| 1-5      | (4)                      | (0, 1, 2, 3, 5)                         | 0.06    |0.00  |\n",
    "| 1-6      | (5)                      | (0, 1, 2, 3, 4)                         | 0.61    |0.11  |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EnvSiamense",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
